Описание работы для конкурса Intel® Acceler8
============================================
:author: Александр Соловец
:toc:

:numbered:

Информация об команде
---------------------
. Название: "GNU is Not Linux"
. Состав: Александр Соловец (link:resume.pdf[резюме])

Описание поставленной задачи
----------------------------

Условия задачи
~~~~~~~~~~~~~~
Задача конкурса состоит в том, чтобы в заданной матрице, состоящей из целых
чисел, найти подматрицу с максимальной возможной суммой её элементов. В данном
случае 'подматрицой' считается матрица образованная из подмножества строк и
столбцов исходной матрицы, идущих подряд. В случае, если ответов может быть
несколько, следует вывести любой из них.

Решение должно быть представленно в виде программы, которая принимает два
параметра -- имена входного и выходного файлов.

Формат входных данных
~~~~~~~~~~~~~~~~~~~~~
Первым элементом входных данных является число *T*, обозначающее количество
тестов. Следом идут *T* строк, содержащих шесть чисел: *N*, *M*, *S*, *a*,
*b*, *m*. Первые два -- число строк и столбцов в исходной матрице, остальные
четыре -- параметры генератора псевдо-случайных чисел.

Исходная матрица *A* строится при помощи следующего алгоритма:

[source,c]
------------------------------------------
int sum = 0;
int seed = seed0;
size_t dstSize = M * N;

int mean, remainder;

for (i = 0; i < dstSize; i += 1) {
	seed = PRNG(seed, a, b, m);
	A[i] = seed;
	sum += seed;
}

mean = sum / (signed) dstSize;
remainder = sum - mean * dstSize;
mean += (remainder * 2 > (signed) dstSize ? (1) : (0);
mean -= (remainder * 2 < -(signed) dstSize ? (1) : (0);

for (i = 0; i < dstSize; i += 1)
{
	A[i] -= mean;
}
------------------------------------------

Значения *seed*, *a*, *b* и *m* берутся из входных данных. Функция +PRNG+
выглядит следующим образом:

[source, c]
------------------------------
PRNG(seed, a, b , m)
{
	return (a * seed + b) % m;
}
------------------------------

Ограничения для входных параметров:

- +1 ≤ *T* ≤ 1000+
- +1 ≤ *N*, *M*, *S*, *a*, *b*, *m* ≤ 20000+

Пример входного файла:

---------------
2
4 3 10 3 4 17
14 31 11 4 5 18
---------------

Формат выходных данных
~~~~~~~~~~~~~~~~~~~~~~
Для каждого теста в выходной файл нужно вывести строку "+Case #x:+ ", за которой
следуют 6 целых чисел (через пробел) и символ переноса строки.

Первые 4 числа — две пары координат, задающих подматрицу с максимальной суммой
элементов (в первой паре оба числа должны быть меньше или равны
соответственным числам во второй паре, т.е. сначала описывается координаты
левого верхнего, а потом правого нижнего угла). В каждой паре первое число --
номер строки, а второе число — номер столбца (нумерация начинается с нуля).
Если вы нашли несколько подматриц с одинаковой максимальной суммой элементов,
выведите координаты любой из них.

Последние 2 числа — сумма элементов в найденной подматрице и площадь этой
подматрицы соответственно.

Весь вывод должен идти в выходной файл, имя которого задается вторым
параметром командной строки.

Пример выходного файла:

----------------------
Case #1: 0 2 3 2 17 4
Case #2: 0 4 5 9 18 36
----------------------

Алгоритм решения задачи
-----------------------

Последовательный алгоритм
~~~~~~~~~~~~~~~~~~~~~~~~~
В качестве основного алгоритма решения задачи был взял модифицированный
http://en.wikipedia.org/wiki/Maximum_subarray_problem#Kadane.27s_algorithm[алгоритм
Джея Кадана].

Исходный алгоритм позволяет найти массив максимальной суммы, содержащийся в
исходном массиве, за линейное время. В двумерном случае перебирают строки (или
столбцы) матрицы и ищут под-массив максимальной суммы в массиве, составленном
из подматрицы, ограниченной с двух сторон границами, соответствующими
перебираемым значениям. Проще говоря, во внешнем двойном цикле перебираются
индексы *i* и *j*, при этом получается одномерный массив для поиска, где *k*-ый элемент
равняется сумме *k*-ых элементов исходной матрицы, находящихся в строках
*i*..*j*. Таким образом сложность алгоритма сравнима с O(*N²M*).

Параллельный алгоритм
~~~~~~~~~~~~~~~~~~~~~
В качестве средств распараллеливания был выбран интерфейс +OpenMP+. Во-первых, этот
интерфейс является переносимым и поддерживается компиляторами +GNU+ и +Intel+
языка программирования +С+. Во-вторых, тратит совсем немного времени на
издержки взаиможействия между потоками.

На <<graph01, первом графике>>footnote:[Здесь и далее графики имеют
логирифмический масштаб, время измеряется в минутах, по оси *X* отложено
количество итераций] показано время начальной версии решения для разного
количества потоков.

[[graph01]]
.Первая рабочая версия.
image::img/data1.png[align="center"]

Первым делом был исправлен большой недостаток -- во внешнем цикле перебирались
не строки, а столбцы. Это приводило к тому, что построение одномерного массива
использовало все строки матрицы. А это, в свою очередь, приводило к постоянной
инвалидации кэша. Поменям вестами размерности, удалось на целый порядок
уменьшить время работы:

.Заголовок.
.Поворот матрицы и удаление лишнего копирования.
image::img/data2.png[align="center"]

Также в этой версии удлось избавиться от промежуточного массива, т.к. внешний
цикл использует каждый элемент ровно один раз.

Исправление еще одного недостатка позволило также умень время работы на
порядок. Он заключался в том, что критическая секция, в которой происходило
обновление результата, раполагался сразу за внутренним циклом. Её смещение к
месту замершения работы потока позволило снизить время, которое простаивали
потоки ожидая выхода друг друга из критической секции.

.Смещение критических секций.
image::img/data3.png[align="center"]

Стоит отметить, что наилучшим решением было бы вобще не использовать
критические секции, а вместо этого сохранять результат локально в элемент
массива по индексу, равному номеру потока. Однако, тесты показали, что такое
решение работает очень медленно по причине +NUMA+-эффекта.

Последним элементом оптимизации стал переход со статического планирования
(_static_) на динимическое (_dynamic_). В +OpenMP+ при статическом планировании в
циклах итерации делятся поровну между всеми доступными потоками в начале
работы. При динамическом -- каждый свободный поток выполняется оприделённое
количество итераций, именуемых _порцией_, после чего снова становится
свободным.

Статическое планирование имеет наименьшее время издержек, но не всегда
способно правильно поделить итерации. В данном случае количество итераций во
втором внешнем цикле запвисит от значения индекса в первом цикле, по которому
как раз и происходит разбиение. Поэтому поделив первый внешний цикл на равные
части, мы придём к тому, что потоки, выполняющие последние итерации будут
завершаться гораздо быстрее потоков, работающих с начальными значениями. При
динамическом планировании этот недостаток исчезает, т.к. освободившийся поток
будет заниматься новой порцией итерций. Рузльтат можно увидеть на следующем
графике:

.Использование динамического планировани, размер порции -- 20.
image::img/data4.png[align="center"]

Важно очень внимательно отнестить к размеру порции. Маленькие значения могут
значительно увеличить время издержек многопоточности, в то время, как большие
значения будут иметь тот же недостаток, что и статическое планирование.

Были составлены графики для различных значений размера порции, и проведён
анализ времени выполнения на разных тестах. Результат можно увидеть на
рисунках 5, 6, 7.

.Размер порции 10.
image::img/data6.png[align="center"]

.Размер порции 30.
image::img/data7.png[align="center"]

.Размер порции 50.
image::img/data8.png[align="center"]

Поскольку опеределить наиболее оптимальное решение, исходя из данных графиков,
довольно трудно, было решено составить график суммарного времени работы
каждой версии и выбрать наилучший:

[[hist01]]
.Суммарное время работы для разных значений размера порции на поток.
image::img/hist01.png["img8",align="center"]

На <<hist01,рисунке 8>> видно, что при значении "20" достигается наилучший результат,
поэтому было решего оставить его в финальной версии.

Анализ достигнутых и предполагаемых результатов
-----------------------------------------------
На основе измерения времени работы параллельного алгортма можно сделать вывод,
что данное решение достаточно эффективно использует интерфейс +OpenMP+ и
позволяет достичь прироста в производительности, сравнимого с линейным.
Оно также доказывает, что помимо правильного использования средств
параллелизации необходимо граммотно организовывать параллельный доступ к
памяти и сводить к минумуму количество записей в общую память.

Для того, чтобы сделать вывод о качестве параллелизации, еще раз обратимся к
графикам. Посчитаем суммарное время работы финальной версии решения для
разного количества потоков и построем гисторгамму этих сумм, предварительно
нормализовав их относительно времени работы при одном потоке.

[[hist02]]
.Прирост производительности при различном количестве котоков.
image::img/hist02.png[align="center"]

На <<hist02,рисунке 9>> видно, что при колиестве потоков до 16 удаётся достичь линейного роста
производительности, который потом постепенно замедляется. При этом при
значениях, близких к 40 (максимальное число ядер в +MTL+) наблюдается резкое
замедление. Пока трудно сказать, чем оно вызвано. Известно лишь несколько
фактов:

* в +MTL+ используется +NUMA+
* такой эффект наблюдали и другие пользователи
* эффект проявляется не всегда


:numbered!:

[appendix]
Исходный код
------------
[source,c,n]
-----------------------
include::../src/msp.c[]
-----------------------
[appendix]
Список использованных инструментов
----------------------------------
- http://gcc.gnu.org/[+GNU C compiler+]
- http://www.gnu.org/s/bash/[+GNU Bash+]
- http://www.perl.org/[+Perl+]
- http://www.methods.co.nz/asciidoc/[+AsciiDoc+]
- http://www.gnuplot.info/[+gnuplot+]
- http://www.gnu.org/s/src-highlite/[+source-highlight+]
- http://www.gnu.org/s/coreutils/[+GNU Coreutils+]
- http://www.gnu.org/s/make/[+GNU Make+]
